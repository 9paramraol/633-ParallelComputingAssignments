

1. The implementation optimization
2. node level
3. group level
4. node + group level
5. rank placements

- We experimented following strategies for the Bcast, Reduce, Gather:

1. Node level optimization only
- In this case, we formed a communicator for all ranks belonging to the same node. Then for each such group, there was a leader ie. for node 1 there is leader 1, node 2 there is leader 2, etc.  
- Then a separate communicator was formed which consisted of these node level leader only which is used for inter-node communication.

2. Group-level optimization only
- In this case, we formed a communicator for all the ranks belonging to the same group (group here is wrt to topology). Then for each group level communicator one group level leader was decided for eg for group 1 there is a leader 1 and so on.
- Then a separate communicator was formed which consisted of the group leaders only which is used for inter-group communication.

3. Node plus Group level optimization
- In this case, the first level of communicator consisted of all the ranks belonging to the same node. Then each of such a node-level communication group has a leader.
- A new communicator at the level of the group was formed which is basically consisted of node-level leaders only. This communicator is used for intra-group communications. Each of the group-level communicators also had a leader which is one of the members of the node level leader of the same group.
- Then these group-level leaders were used to create a communicator which is then used for inter-group communication.

4. Rank placements optimization
- In this method instead of creating multiple communicators a single communicator is created which has the following properties:
	a. All the processes belonging to the node have contagious ranks in the new communicator.
	b. All the processes belonging to the same group are st they occupy a contagious set of ranks. That is for eg 2 to 8 rank processes belong to the same group then none of the processes having a rank between 2 to 8 in the new communicator will belong to other groups.
- And then the collective call is called on the new communicator. 

For Gather, Bcast, Reduce time for all the methods described were observed for various ppn(0,8), a number of nodes(4,16), different data sizes 16KB, 256KB, and 2048KB. Based on the times observed for different methods we made the decision to call different optimization ie. node lvl only, or group level only, etc. based on the parameter viz. ppn, data-size, and a number of processes, to get the best performance. 

An implementation optimization worth noting is where we assigned the key of the root for any collective as (-1)*root_rank while calling the Comm_Split. As all other keys were greater than equal to zero while forming a group at any level so by construction the new rank of the root for the new communicator should be 0 hence making it the leader of any communicator it is in. So all the unnecessary communications that were pertaining to the finding of root in our initial implementations were redundant thus shaving off some communication time. 

We also experimented with different host files. Basically in the normal host file generated by the helper function provided, all the contagious ranks are placed on the same node and the nodes belonging to the same group also have their ranks placed in such a manner that often many of the communications are done at the lowest level possible. So we looked at the cases where every process is placed in the most randomized manner possible:

For eg for the host file
csews1:2
csews31:2

the corresponding host file that we created was
csews1:1
csews31:1
csews1:1
csews:31

For these host files, the topology-aware algorithm either changed the ranks(in 4) or did communications by exploiting the topology levels(1,2 and 3) hence ensuring that the communications were done in the most optimized manner possible. However, the default topology agnostic collective calls indeed performed worse as compared to the optimizations. But since the ppn is often given as the correct parameter ie. csews1:1, csews:1 is usually written as csews1:2 by the used we did not use these host files for our results. Instead, we just randomized the lines of the host-files generated which is quite a practical case when one does not know the topology of the network. Basically, the transformations that we made while generating results just shuffled the lines of the host file that is generated hence ensuring that the node in the same groups is not necessarily contiguous in the host file ie.

csews1:4
csews2:4
csews31:4
csews32:4 

was randomly shuffled to :

csews2:4
csews31:4
csews1:4
csews:32:4

###--------------------- Bcast

Before going forward remember that the root will always be the leader at the topmost level ie. it will have 0 rank in any group. A brief algorithm for various optimizations in the case of Bcast is provided below:

1. The node-level optimization pseudo code will be:
	a. First broadcast the buffer to the node leaders from the root.
	b. After the previous broadcast the buffer received by the node leaders to the elements of the node communicator.
2. The group-level optimization pseudo code will be:
	a. First broadcast the buffer to the group leaders from the root.
	b. After the previous broadcast the buffer received by the group leaders to the elements of the group communicator.
3. The node + group level optimization pseudo code will be.
	a. First broadcast the buffer to the group leaders from the root.
	b. Then the group leader will broadcast the buffer to the node leaders.
	c. Then finally the node leaders will broadcast the buffer to the node ranks in the communicator.
4. The rank optimization will be.
	a. form a new group with following key given to the Comm_split, (10000*group_number + 100*node_number - comm_world_rank) (except the actull root whose key is set to 0). 
	b. Note that the color is the same for all ranks in Comm_split, and node_number is 1 for csews1 and so on. The subtraction of comm_world rank is just done for distinct keys it is a small number and ensures that the rank level optimization constraints are satisfied for the new communicator formed.
	c. Then the broadcast is called from the root to all other processes using the new communicator.

We ran all the methods for all the configurations ie. different data size, ppn, nodes and then after observing the best method for each set of configuration, (A conifiguration is of form (# processes, ppn, data size)) for multiple iterations. Based on the following results we decide that we should use the bcast in following manner:

	if(numProcs/ppn > 13 && dataSize >= 2048KB )
		MPI_Bcast_opt_group_only
	else
		MPI_Bcast_opt_rank_only

The reasoning for this was
The results are as follows:

###------------------ Reduce


A brief idea of algorithms used for various levels of optimizations in the case of Reduce is provided below:
1. The node-level optimization pseudo code will be:
	a. Reduce all the processes in the same rank and set the root as the node level group's leader.
	b. Use the reduced buffer of the node leaders, which is further reduced amongst themselves using the inter-node communicator (of node leaders), and set the root as the actual root.
	c. From construction root will always be the node leader.
2. The group-level optimization pseudo code will be:
	a. Reduce all the processes in the same group and set the root as the group leader.
	b. Use the reduced buffer of the group leaders, which is further reduced amongst themselves using the inter-group communicator (of group leaders) and set the root as the actual root.
	c. From construction root will always be the group leader.
3. The group+node level optimization pseudo code will be:
	a. Reduce all the processes in the same rank and set the root as the node level group's leader.
	b. Now use these reduced buffers for further reduction at the group level composed of node leaders and set the root at the corresponding group leader.
	c. Now reduce the buffer received at the group leaders and reduce them using the inter-group communicator, and set the root as the actual root.
	d. From construction root will always be the group leader and a node leader.
4. Rank optimization will be:
	a. form a new group with the following key given to the Comm_split, (10000*group_number + 100*node_number - comm_world_rank) (except the actual root whose key is set to 0). 
	b. Note that the color is the same for all ranks in Comm_split, and node_number is 1 for csews1 and so on. The subtraction of comm_world rank is just done for distinct keys it is a small number and ensures that the rank level optimization constraints are satisfied for the new communicator formed.
	c. Then the reduction is called from the root to all other processes using the new communicator.

We ran all the methods for all the configurations ie. different data size, ppn, nodes and then after observing the best method for each set of configuration, (A conifiguration is of form (# processes, ppn, data size)) for multiple iterations. Based on the following results we decide that we should use the reduce in following manner:

	if(ppn > 1){
		if( data_size < 2048KB)
			MPI_Reduce_opt_node_only
		else
			MPI_Reduce_opt_node_group
	}
	else{
		if( count < 2048KB)
			MPI_Reduce_opt_rank_only
		else
			MPI_Reduce_opt_group_only
	}

The reasoning for this was
The results are as follows:

###---------------- Gather

A brief description of the algorithm used for various optimization levels is provided below:

1. The node-level optimization:
	a. First gather all the data at the node-leader for the ranks in the node level communicator.
	b. Then used the inter-node level communicator to gather all the data collected by the node leaders at the root.
	c. Gather of rank positions of the buffer was also done so now un-shuffle the buffer's data using the rank positions gathered.
2. The group-level optimization:
	a. First gather all the data at the group-leader for the ranks in the group level communicator.
	b. Then used the inter-group level communicator to gather all the data collected by the group leaders at the root.
	c. Gather of rank positions of the buffer was also done so now un-shuffle the buffer's data using the rank positions gathered.
3. The node+group level optimization:
	a. First gather all the data at the node-leader for the ranks in the node level communicator.
	b. Now gather the buffers at the node leaders at the group level to the group leaders.
	c. Now gather the data from the group leaders to the root and unshuffle the data at the root.
4. Rank optimization will be:
	a. form a new group with the following key given to the Comm_split, (10000*group_number + 100*node_number - comm_world_rank) (except the actual root whose key is set to 0). 
	b. Note that the color is the same for all ranks in Comm_split, and node_number is 1 for csews1 and so on. The subtraction of comm_world rank is just done for distinct keys it is a small number and ensures that the rank level optimization constraints are satisfied for the new communicator formed.
	c. Then the gather is called from the root to all other processes using the new communicator. After that, the de-shuffling of the buffer is done.


Note: We have also written implementations where the number of nodes in a group is variable however after reading the post that we could take ppn as the input we assumed that the nodes per group could also be assumed constant as otherwise extra gathers of vector form have to be done to get data in various groups.

We ran all the methods for all the configurations ie. different data sizes, ppn, nodes, and then after observing the best method for each set of configurations, (A configuration is of the form (# processes, ppn, data size)) for multiple iterations. Based on the following results we decide that we should use the gather in the following manner:

	if(numProcs/ppn > 13)
		MPI_Gather_opt_group_only
	else
		MPI_Gather_opt_rank_only
		
The reasoning for this was
The results are as follows: